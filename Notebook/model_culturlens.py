# -*- coding: utf-8 -*-
"""Model_CulturLens.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1quNNNEnZqfNMCp6eyofDygt52Ion8G-y

## Instal Dataset
"""

! gdown --id 1Q_-lGZUO2Xkao3APiXpClak6V-XQ0MqP

"""Extrac Dataset"""

import zipfile

local_zip = '/content/dataset_CulturLens.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall()

zip_ref.close()

"""## Import Libary and requirement"""

import io
import numpy as np
import os
import tensorflow_hub as hub
import tensorflow as tf
import matplotlib.pyplot as plt
import matplotlib.image as mpimg



from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow import keras
from PIL import Image
from google.colab import files

"""## Preparation Data"""

dataset_path = '/content/dataset_CulturLens'

import os

penjor_dir = os.path.join(dataset_path, 'Penjor')
Canang_dir = os.path.join(dataset_path, 'Canang')
Pelinggih_dir = os.path.join(dataset_path, 'Pelinggih')
Pelangkiran_dir = os.path.join(dataset_path, 'Pelangkiran')
Gebogan_dir = os.path.join(dataset_path, 'Gebogan')
BantenSaiban_dir = os.path.join(dataset_path, 'banten_saiban')
kain_Poleng_dir = os.path.join(dataset_path, 'kain poleng')

print(f'total penjor images: {len(os.listdir(penjor_dir))}')
print(f'total Canang images: {len(os.listdir(Canang_dir))}')
print(f'total Pelinggih images: {len(os.listdir(Pelinggih_dir))}')
print(f'total Pelangkiran images: {len(os.listdir(Pelangkiran_dir))}')
print(f'total Gebogan images: {len(os.listdir(Gebogan_dir))}')
print(f'total BantenSaiban images: {len(os.listdir(BantenSaiban_dir))}')
print(f'total Kain Poleng images: {len(os.listdir(kain_Poleng_dir))}')

"""## Augmentasi Data"""

image_size = (224, 224)
batch_size = 64

datagen = ImageDataGenerator(
    rescale=1.0/255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.3
)

train_generator = datagen.flow_from_directory(
    dataset_path,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='training'
)

validation_generator = datagen.flow_from_directory(
    dataset_path,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation'
)

images, labels = next(train_generator)

for i in range(9):
    plt.subplot(3, 3, i+1)
    plt.imshow(images[i])
    plt.axis('off')
plt.show()

"""## Create A Model with MobilNetV2"""

pre_trained_model = MobileNetV2(weights="imagenet", include_top=False,
                                input_tensor=Input(shape=(224, 224, 3)))

for layer in pre_trained_model.layers:
    layer.trainable = False

last_output = pre_trained_model.output

x = tf.keras.layers.Flatten(name="flatten")(last_output)
x = tf.keras.layers.Dropout(0.5)(x)
x = tf.keras.layers.Dense(128, activation="relu")(x)
x = tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')(x)

model = tf.keras.models.Model(pre_trained_model.input, x)

model.summary()

model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(
    train_generator,
    epochs=10,
    validation_data=validation_generator,)

"""## Traning Data"""

#-----------------------------------------------------------
# Retrieve a list of list results on training and test data
# sets for each training epoch
#-----------------------------------------------------------
acc      = history.history[     'accuracy' ]
val_acc  = history.history[ 'val_accuracy' ]
loss     = history.history[    'loss' ]
val_loss = history.history['val_loss' ]

epochs   = range(len(acc)) # Get number of epochs

#------------------------------------------------
# Plot training and validation accuracy per epoch
#------------------------------------------------
plt.plot(epochs, acc, label='Training Accuracy')
plt.plot(epochs, val_acc, label='Validation Accuracy')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()

#------------------------------------------------
# Plot training and validation loss per epoch
#------------------------------------------------
plt.plot(epochs, loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.title('Training and validation loss')
plt.legend()

"""## Save Model"""

os.makedirs("/content/Save_model", exist_ok=True)

model.save("/content/Save_model/model.h5")

"""### Create Label For Tf.lite"""

labels = [
    "Canang",
    "Gebogan",
    "Pelangkiran",
    "Pelinggih",
    "Penjor",
    "Banten Saiban",
    "Kain Poleng"
]

with open("labels.txt", "w") as file:
    for label in labels:
        file.write(label + "\n")

print("File labels.txt Succsesfull!")

"""## Convert to TF.lite

instal tflite support
"""

pip install tflite-support

h5_model_path = "/content/Save_model/model.h5"
model = tf.keras.models.load_model(h5_model_path)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

tflite_model_path = "model.tflite"
with open(tflite_model_path, "wb") as f:
    f.write(tflite_model)

print("Model Succsesfull convert to:", tflite_model_path)

"""## Put a Metadata for Tf.lite"""

!pip install --upgrade protobuf==3.20.3

from tflite_support.metadata_writers import image_classifier
from tflite_support.metadata_writers import writer_utils

# Path file TFLite Anda
model_path = "/content/model.tflite"
output_model_path = "/content/Save_model/model_CulturLens.tflite"

# Detail metadata
input_mean = [127.5, 127.5, 127.5]
input_std = [127.5, 127.5, 127.5]
label_file = "labels.txt"


writer = image_classifier.MetadataWriter.create_for_inference(
    writer_utils.load_file(model_path),
    input_mean,
    input_std,
    label_file_paths=[label_file]
)

metadata_model = writer.populate()
writer_utils.save_file(metadata_model, output_model_path)

print("Metadata berhasil ditambahkan ke model:", output_model_path)

"""## Testing A Model h5"""

def preprocess_image(image_path, target_size):

    img = load_img(image_path, target_size=target_size)
    img_array = img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array /= 255.0
    return img_array

custom_objects = {'KerasLayer': hub.KerasLayer}

model = tf.keras.models.load_model('/content/Save_model/model.h5', custom_objects=custom_objects)

class_indices = {
    0: 'Canang',
    1: 'Gebogan',
    2: 'Pelangkiran',
    3: 'Pelinggih',
    4: 'Penjor',
    5: 'Banten Saiban',
    6: 'kain poleng'
}

uploaded = files.upload()

for file_name in uploaded.keys():
    print(f"File diunggah: {file_name}")
    image_path = file_name

    img = mpimg.imread(image_path)
    plt.imshow(img)
    plt.axis('off')
    plt.title(f"Gambar: {file_name}")
    plt.show()

    image = preprocess_image(image_path, target_size=(224, 224))
    predictions = model.predict(image)
    predicted_class = np.argmax(predictions[0])
    predicted_probability = predictions[0][predicted_class] * 100
    predicted_label = class_indices[predicted_class]

    print(f"The uploaded image is predicted as: {predicted_probability:.2f}% {predicted_label}")